{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127321bf",
   "metadata": {},
   "source": [
    "# MLP transient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89305a8e",
   "metadata": {},
   "source": [
    "## Previous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa7860",
   "metadata": {},
   "source": [
    "Para que funcione en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo: Local\n",
      "Ruta datasets: C:\\Users\\ismael.gallo/ia_thermal_colab\\datasets\n",
      "Ruta modelos: C:\\Users\\ismael.gallo/ia_thermal_colab\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "# Detectar si estamos en Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Ruta base\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = \"/content/drive/MyDrive/ia_thermal_colab\"\n",
    "else:\n",
    "    BASE_PATH = os.path.expanduser(\"~/ia_thermal_colab\")\n",
    "\n",
    "DATASETS_PATH = os.path.join(BASE_PATH, \"datasets\")\n",
    "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
    "\n",
    "os.makedirs(DATASETS_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Modo:\", \"Colab\" if IN_COLAB else \"Local\")\n",
    "print(\"Ruta datasets:\", DATASETS_PATH)\n",
    "print(\"Ruta modelos:\", MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Par√°metros del repo\n",
    "GIT_REPO_URL = \"https://github.com/ismaelgallolopez/ia_thermal.git\"  # üëà Cambia esto\n",
    "REPO_NAME = GIT_REPO_URL.split(\"/\")[-1].replace(\".git\", \"\")\n",
    "CLONE_PATH = os.path.join(BASE_PATH, REPO_NAME)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # üß¨ Clonar el repositorio si no existe ya\n",
    "    if not os.path.exists(CLONE_PATH):\n",
    "        !git clone {GIT_REPO_URL} {CLONE_PATH}\n",
    "    else:\n",
    "        print(f\"Repositorio ya clonado en: {CLONE_PATH}\")\n",
    "\n",
    "    # üì¶ Instalar requirements.txt\n",
    "    req_path = os.path.join(CLONE_PATH, \"requirements.txt\")\n",
    "    if os.path.exists(req_path):\n",
    "        !pip install -r {req_path}\n",
    "    else:\n",
    "        print(\"No se encontr√≥ requirements.txt en el repositorio.\")\n",
    "\n",
    "    print(\"üîÑ Reinicia el entorno para aplicar los cambios...\")\n",
    "    IPython.display.display(IPython.display.Javascript('''google.colab.restartRuntime()'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7dd21",
   "metadata": {},
   "source": [
    "Importaci√≥n de librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import optuna\n",
    "from typing import Sequence, Union, Optional\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# get the directory path of the file\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal\")\n",
    "\n",
    "from plot_functions import *\n",
    "from Physics_Loss import *\n",
    "from utils import *\n",
    "from scripts.PCB_solver_tr import *\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/ismaelgallo\")\n",
    "\n",
    "# from architectures.mlp_pcb import *\n",
    "\n",
    "from Dataset_Class_mlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a85f2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be876f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_specs = {\n",
    "    \"os\": platform.system(),  # e.g. \"Linux\", \"Windows\", \"Darwin\"\n",
    "    \"os_version\": platform.version(),\n",
    "    \"machine\": platform.machine(),  # e.g. \"x86_64\"\n",
    "    \"processor\": platform.processor(),  # e.g. \"Intel64 Family 6 Model 158\"\n",
    "    \"python_version\": platform.python_version(),\n",
    "    \"device\": str(device)\n",
    "}\n",
    "if torch.cuda.is_available():\n",
    "    system_specs[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    system_specs[\"gpu_memory_total_GB\"] = round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)\n",
    "    system_specs[\"cuda_version\"] = torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2a2e",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37fed6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "n_test = 200\n",
    "n_val = 20\n",
    "time_sim = 1000 # seconds\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "sequence_length = time_sim+1 # seconds\n",
    "dt = 1 # seconds\n",
    "T_init = 298.0 # Kelvin\n",
    "nodes_side = 13 # number of nodes in one side of the PCB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e01f5",
   "metadata": {},
   "source": [
    "Dataset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c033746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cargando mlp transient dataset base desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_mlp_transient_dataset.pth\n",
      "Cargando dataset base desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_mlp_transient_dataset_train.pth\n",
      "Cargando dataset base desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_mlp_transient_dataset_val.pth\n",
      "Cargando dataset base desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_mlp_transient_dataset_test.pth\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "  dir_path = BASE_PATH\n",
    "\n",
    "base_dataset = load_dataset_mlp(base_path=dir_path)  # ‚Üê carga el dataset base completo (PCB_mlp_transient_dataset.pth)\n",
    "dataset_train = load_trimmed_dataset_mlp(base_path=dir_path, dataset_type='train', max_cases=n_train, to_device=True)\n",
    "dataset_val = load_trimmed_dataset_mlp(base_path=dir_path, dataset_type='val', max_cases=n_val, to_device=True)\n",
    "dataset_test = load_trimmed_dataset_mlp(base_path=dir_path, dataset_type='test', max_cases=n_test, to_device=True)\n",
    "\n",
    "loader = DataLoader(base_dataset, batch_size=batch_size, shuffle=False)  # DEBUGGING\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62770947",
   "metadata": {},
   "source": [
    "Debugging para comprobar que cumpla tama√±os que nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3084bc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 10]), torch.Size([1000, 169]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade0c21",
   "metadata": {},
   "source": [
    "## Hyperparameters of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c63d188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "lr = 1e-2\n",
    "lrdecay = 0.01\n",
    "lrdecay_patience = 25\n",
    "early_stop_patience = 100\n",
    "\n",
    "hidden_layers = [64, 64, 64, 64, 64]\n",
    "activation = nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7f1ac",
   "metadata": {},
   "source": [
    "# Non-physics MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663aab6",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "87c0b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_PCB(nn.Module):\n",
    "    def __init__(self, input_dim:int=10, output_dim:int=169, hidden_layers=[128, 128], activation=nn.SiLU):\n",
    "        \"\"\"\n",
    "        Modelo MLP configurable para regresi√≥n.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): N√∫mero de caracter√≠sticas de entrada.\n",
    "            output_dim (int): N√∫mero de salidas (por ejemplo, nodos de temperatura).\n",
    "            hidden_layers (list): Lista con el n√∫mero de neuronas por capa oculta.\n",
    "            activation (callable): Clase de funci√≥n de activaci√≥n (por defecto nn.ReLU).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(activation())\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4ff8073f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_PCB(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=64, out_features=169, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP_PCB(hidden_layers=hidden_layers, activation=activation).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lrdecay, patience=lrdecay_patience, verbose=True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f4459",
   "metadata": {},
   "source": [
    "Ensure data is moved to the appropriate device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ba349ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x est√° en: cuda:0\n",
      "y est√° en: cuda:0\n",
      "model est√° en: cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "\n",
    "print(f\"x est√° en: {x.device}\")\n",
    "print(f\"y est√° en: {y.device}\")\n",
    "print(f\"model est√° en: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c93ae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae59c8",
   "metadata": {},
   "source": [
    "Training and saving best model with best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1acd8424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‚ú® Nuevo entrenamiento: `PCB_MLP_nt1000_e500_lr0.01_bs1000_h-64-64-64-64-64_SiLU.pth`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Los resultados se guardar√°n en: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\models\\MLP_PCB\\PCB_MLP_nt1000_e500_lr0.01_bs1000_h-64-64-64-64-64_SiLU.json\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss_hist = []\n",
    "best_val_loss = np.inf\n",
    "epochs_without_improvement = 0\n",
    "elapsed_previous = 0.0  # minutos ya entrenados en sesiones anteriores\n",
    "\n",
    "layer_string = \"\"\n",
    "for d in hidden_layers:\n",
    "    layer_string += f\"-{d}\"\n",
    "    \n",
    "activation_string = activation.__name__\n",
    "\n",
    "model_dir = os.path.join(dir_path, 'models', 'MLP_PCB')\n",
    "if IN_COLAB:\n",
    "    model_dir = os.path.join(MODELS_PATH, 'MLP_PCB')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "filename = f\"PCB_MLP_nt{n_train}_e{epochs}_lr{lr}_bs{batch_size}_h{layer_string}_{activation_string}.pth\"\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "json_path  = model_path.replace('.pth', '.json')\n",
    "\n",
    "start_epoch = 0\n",
    "start_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# --- L√≥gica de reanudaci√≥n ---\n",
    "if os.path.exists(model_path):\n",
    "    display(Markdown(f\"üîÅ Cargando modelo existente `{filename}` y reanudando entrenamiento.\"))\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            prev = json.load(f)\n",
    "        best_val_loss         = prev.get('best_val_loss', best_val_loss)\n",
    "        train_loss            = prev.get('train_loss', [])\n",
    "        val_loss_hist             = prev.get('val_loss', [])\n",
    "        elapsed_previous      = prev.get('training_duration_minutes', 0.0)\n",
    "        start_datetime        = prev.get('start_datetime', start_datetime)\n",
    "        start_epoch           = prev.get('epochs_trained', 0)\n",
    "        # Reiniciamos el contador de paciencia tras mejora\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        display(Markdown(\"‚ö†Ô∏è No se encontr√≥ JSON de hiperpar√°metros; se reinicia contadores.\"))\n",
    "else:\n",
    "    display(Markdown(f\"‚ú® Nuevo entrenamiento: `{filename}`\"))\n",
    "\n",
    "print(\"üìÅ Los resultados se guardar√°n en:\", json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3b8e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπÔ∏è Early stopping en la √©poca 130 tras 100 √©pocas sin mejora.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for x, y in train_bar:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_loss.append(epoch_loss)\n",
    "    # print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    # Validaci√≥n\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_bar:\n",
    "            y_pred_val = model(x_val)\n",
    "            loss_val = criterion(y_pred_val, y_val)\n",
    "            val_loss += loss_val.item() * x_val.size(0)\n",
    "            val_bar.set_postfix(val_loss=loss_val.item())\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    # print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.6f}\")\n",
    "    # If you want to keep a history, use a list like val_loss_hist and append here:\n",
    "    val_loss_hist.append(val_loss)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        # Guardar JSON con info de entrenamiento\n",
    "        training_info = {\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"epochs_trained\": epoch + 1,\n",
    "            \"training_duration_minutes\": elapsed_previous + ((datetime.now() - datetime.strptime(start_datetime, \"%Y-%m-%d %H:%M:%S\")).total_seconds() / 60.0),\n",
    "            \"start_datetime\": start_datetime,\n",
    "            \"hyperparameters\": {\n",
    "                \"epochs\": epochs,\n",
    "                \"lr0\": lr,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"hidden_layers\": layer_string,\n",
    "                \"activation\": activation_string,\n",
    "            },\n",
    "            \"final lr\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss_hist,\n",
    "            # \"train_loss\": train_loss,\n",
    "            # \"val_loss\": val_loss_hist\n",
    "        }\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(training_info, f, indent=2)\n",
    "        # print(f\"‚úÖ Modelo guardado: {model_path}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= early_stop_patience:\n",
    "        print(f\"‚èπÔ∏è Early stopping en la √©poca {epoch+1} tras {early_stop_patience} √©pocas sin mejora.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020559b",
   "metadata": {},
   "source": [
    "Plot training loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "73f8efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_evolution(train_loss, val_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa79576",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd83e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismael.gallo\\AppData\\Local\\Temp\\ipykernel_19300\\3240133991.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model = MLP_PCB(hidden_layers=hidden_layers, activation=activation).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Test\n",
    "test_loss = 0.0\n",
    "test_bar = tqdm(test_loader, desc=f\"Test\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in test_bar:\n",
    "        y_pred_test = model(x_test)\n",
    "        loss_test = criterion(y_pred_test, y_test)\n",
    "        test_loss += loss_test.item() * x_test.size(0)\n",
    "        test_bar.set_postfix(test_loss=loss_test.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeea70c",
   "metadata": {},
   "source": [
    "## Optuna study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ad1c1",
   "metadata": {},
   "source": [
    "Estudio para encontrar las mejores combinaciones de numero de capas con iteraciones en n√∫mero de neuronas ocultas, learning rate, numero de capas, funcion de activacion, lr decay factor y patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1bf04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Sugerir hiperpar√°metros\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
    "    activation_name = trial.suggest_categorical('activation', ['ReLU', 'SiLU', 'Tanh'])\n",
    "    activation = getattr(nn, activation_name)\n",
    "    lrdecay = trial.suggest_loguniform('lrdecay', 0.1, 0.5)\n",
    "    lrdecay_patience = trial.suggest_int('lrdecay_patience', 5, 50)\n",
    "\n",
    "    # Definir arquitectura\n",
    "    hidden_layers = [hidden_dim] * n_layers\n",
    "    model = MLP_PCB(hidden_layers=hidden_layers, activation=activation).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lrdecay, patience=lrdecay_patience, verbose=False)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    epochs_optuna = epochs\n",
    "    train_loss = []\n",
    "    val_loss_hist = []\n",
    "    start_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    for epoch in range(epochs_optuna):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss.append(epoch_loss)\n",
    "        # Validaci√≥n\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                y_pred_val = model(x_val)\n",
    "                loss_val = criterion(y_pred_val, y_val)\n",
    "                val_loss += loss_val.item() * x_val.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Guardar modelo y JSON en carpeta optuna_studies/models\n",
    "    os.makedirs(\"optuna_studies/mlp_models\", exist_ok=True)\n",
    "    model_path = f\"optuna_studies/mlp_models/model_trial_{trial.number}.pth\"\n",
    "    json_path = f\"optuna_studies/mlp_models/model_trial_{trial.number}.json\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Guardar tambi√©n con el mismo formato y ruta que el entrenamiento normal\n",
    "    layer_string = \"\"\n",
    "    for d in hidden_layers:\n",
    "        layer_string += f\"-{d}\"\n",
    "    activation_string = activation.__name__\n",
    "    # Abreviar el learning rate para el nombre del archivo\n",
    "    lr_str = f\"{lr:.0e}\" if lr < 1e-2 else f\"{lr:.2f}\"\n",
    "    model_dir = os.path.join(dir_path, 'models', 'MLP_PCB')\n",
    "    if IN_COLAB:\n",
    "        model_dir = os.path.join(MODELS_PATH, 'MLP_PCB')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    filename = f\"PCB_MLP_trial{trial.number}_nt{n_train}_e{epochs_optuna}_lr{lr_str}_bs{batch_size}_h{layer_string}_{activation_string}.pth\"\n",
    "    model_path_normal = os.path.join(model_dir, filename)\n",
    "    json_path_normal = model_path_normal.replace('.pth', '.json')\n",
    "    torch.save(model.state_dict(), model_path_normal)\n",
    "\n",
    "    training_info = {\n",
    "        \"trial_number\": trial.number,\n",
    "        \"final_val_loss\": val_loss_hist[-1] if val_loss_hist else None,\n",
    "        \"epochs_trained\": epochs_optuna,\n",
    "        \"training_duration_minutes\": None,  # Puedes calcularlo si quieres\n",
    "        \"start_datetime\": start_datetime,\n",
    "        \"hyperparameters\": {\n",
    "            \"lr\": lr,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"activation\": activation_name,\n",
    "            \"lrdecay\": lrdecay,\n",
    "            \"lrdecay_patience\": lrdecay_patience,\n",
    "        },\n",
    "        \"final_lr\": optimizer.param_groups[0]['lr'],\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss_hist\n",
    "    }\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "    with open(json_path_normal, 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "\n",
    "    return val_loss_hist[-1] if val_loss_hist else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7052c36",
   "metadata": {},
   "source": [
    "Crear y ejecutar el estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a6a1f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 12:30:13,001] Using an existing study with name 'mlp_pcb_study' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "# Crea el estudio y lo guarda en un archivo SQLite\n",
    "os.makedirs(\"optuna_studies\", exist_ok=True)\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name=\"mlp_pcb_study\",\n",
    "    storage=\"sqlite:///optuna_studies/mlp_pcb_optuna_study.db\",\n",
    "    load_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ef7781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismael.gallo\\AppData\\Local\\Temp\\ipykernel_19300\\2447052604.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
      "C:\\Users\\ismael.gallo\\AppData\\Local\\Temp\\ipykernel_19300\\2447052604.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lrdecay = trial.suggest_loguniform('lrdecay', 0.1, 0.5)\n",
      "c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[W 2025-05-19 12:30:17,504] Trial 67 failed with parameters: {'lr': 0.005556181905674362, 'n_layers': 5, 'hidden_dim': 128, 'activation': 'SiLU', 'lrdecay': 0.3895254932972735, 'lrdecay_patience': 25} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\ismael.gallo\\AppData\\Local\\Temp\\ipykernel_19300\\2447052604.py\", line 25, in objective\n",
      "    for x, y in train_loader:\n",
      "  File \"c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-19 12:30:17,506] Trial 67 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[59], line 25\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     24\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     27\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study.optimize(objective, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad3d8dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de ensayo ganador : 41\n",
      "Valor objetivo (p√©rdida) : 1.0661789418519619e-07\n",
      "Hiperpar√°metros √≥ptimos  : {'lr': 0.007992966235711838, 'n_layers': 5, 'hidden_dim': 64, 'activation': 'SiLU', 'lrdecay': 0.3961551271796536, 'lrdecay_patience': 22}\n"
     ]
    }
   ],
   "source": [
    "best_trial   = study.best_trial\n",
    "best_params  = best_trial.params         # dict con los hiperpar√°metros ganadores\n",
    "print(\"N√∫mero de ensayo ganador :\", best_trial.number)\n",
    "print(\"Valor objetivo (p√©rdida) :\", best_trial.value)\n",
    "print(\"Hiperpar√°metros √≥ptimos  :\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b710783",
   "metadata": {},
   "source": [
    "### Loading best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c838a2",
   "metadata": {},
   "source": [
    "Cargar el estudio que mejores resultados obtuvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31c2f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.load_study(\n",
    "    study_name=\"mlp_pcb_study\",\n",
    "    storage=\"sqlite:///optuna_studies/mlp_pcb_optuna_study.db\"\n",
    ")\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d979c",
   "metadata": {},
   "source": [
    "Reconstrucci√≥n de la ruta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0317ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"optuna_studies/mlp_models/model_trial_{best_trial.number}.pth\"\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"No existe el fichero {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3068637",
   "metadata": {},
   "source": [
    "Asignando valors de los par√°metros del mejor modelo de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d343742",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [best_params[\"hidden_dim\"]] * best_params[\"n_layers\"]\n",
    "activation = getattr(torch.nn, best_params[\"activation\"])\n",
    "model = MLP_PCB(hidden_layers=hidden_layers, activation=activation).to(\"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=best_params[\"lr\"])\n",
    "\n",
    "# 4) Crea un nuevo scheduler id√©ntico al usado en el trial\n",
    "scheduler = ReduceLROnPlateau(optimizer,\n",
    "                              mode=\"min\",\n",
    "                              factor=best_params[\"lrdecay\"],\n",
    "                              patience=best_params[\"lrdecay_patience\"],\n",
    "                              verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a020d3",
   "metadata": {},
   "source": [
    "Cargar los pesos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a30d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismael.gallo\\AppData\\Local\\Temp\\ipykernel_19300\\2633090818.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP_PCB(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): SiLU()\n",
       "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (9): SiLU()\n",
       "    (10): Linear(in_features=64, out_features=169, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49023b2",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_heaters = np.ndarray([1 1 1 1])\n",
    "T_interfaces = np.ndarray([290 290 290 290])\n",
    "T_env = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_temperature_mlp(\n",
    "#     model: torch.nn.Module,\n",
    "#     dataset: PCBDataset_mlp,\n",
    "#     Q_heaters: Sequence[float],\n",
    "#     T_interfaces: Sequence[float],\n",
    "#     T_env: float,\n",
    "#     time_raw_seq: Optional[Union[np.ndarray, torch.Tensor]] = None,\n",
    "#     n_steps: int = 1001,\n",
    "#     device: Optional[torch.device] = None\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Inferencia completa de la evoluci√≥n t√©rmica de la PCB.\n",
    "\n",
    "#     Si `time_raw_seq` es None, se crea autom√°ticamente el vector\n",
    "#     de tiempo normalizado de longitud 1001 por defecto.\n",
    "\n",
    "#     Devuelve un array (n_steps, 169) con los mapas de temperatura\n",
    "#     desnormalizados.\n",
    "#     \"\"\"\n",
    "#     # 0) Device y modelo  \n",
    "#     if device is None:\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device).eval()\n",
    "\n",
    "#     # 1) Generar o convertir la secuencia de tiempo cruda\n",
    "#     if time_raw_seq is None:\n",
    "#         time_raw = torch.arange(0, n_steps, dtype=torch.float32)  # (n_steps,)\n",
    "#     else:\n",
    "#         if isinstance(time_raw_seq, np.ndarray):\n",
    "#             time_raw = torch.from_numpy(time_raw_seq.astype(np.float32))\n",
    "#         else:\n",
    "#             time_raw = time_raw_seq.clone().float()\n",
    "#     time_raw = time_raw.to(device)\n",
    "\n",
    "#     # 2) Crear todos los inputs normalizados\n",
    "#     inputs = []\n",
    "#     for t in time_raw.cpu().numpy():\n",
    "#         inp = dataset.create_input_from_values(\n",
    "#             Q_heaters=Q_heaters,\n",
    "#             T_interfaces=T_interfaces,\n",
    "#             T_env=T_env,\n",
    "#             time=t\n",
    "#         )\n",
    "#         inputs.append(inp.unsqueeze(0))\n",
    "#     X = torch.cat(inputs, dim=0).to(device)  # (n_steps, 10)\n",
    "\n",
    "#     # 3) Inferir en batch\n",
    "#     with torch.no_grad():\n",
    "#         preds_norm = model(X)  # (n_steps, 169)\n",
    "\n",
    "#     # 4) Desnormalizar la salida\n",
    "#     temps_denorm = dataset.denormalize_output(preds_norm)  # (n_steps,169)\n",
    "#     return temps_denorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650506a6",
   "metadata": {},
   "source": [
    "Prediction from the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_pred = predict_temperature_mlp(\n",
    "    model=model,\n",
    "    dataset=base_dataset,\n",
    "    Q_heaters=[1, 1, 1, 1],\n",
    "    T_interfaces=[280, 280, 280, 280],\n",
    "    T_env=298.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222ecf4",
   "metadata": {},
   "source": [
    "Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_true, _, _, _ = PCB_case_2(solver='transient', Q_heaters=Q_heaters,  T_interfaces=T_interfaces,  T_env=T_env, time_sim=time_sim, dt=1, nodes_side=13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismael_minimal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
