{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127321bf",
   "metadata": {},
   "source": [
    "# MLP transient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa7860",
   "metadata": {},
   "source": [
    "Para que funcione en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802a673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo: Local\n",
      "Ruta datasets: C:\\Users\\ismael.gallo/ia_thermal_colab\\datasets\n",
      "Ruta modelos: C:\\Users\\ismael.gallo/ia_thermal_colab\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "# Detectar si estamos en Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Ruta base\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = \"/content/drive/MyDrive/ia_thermal_colab\"\n",
    "else:\n",
    "    BASE_PATH = os.path.expanduser(\"~/ia_thermal_colab\")\n",
    "\n",
    "DATASETS_PATH = os.path.join(BASE_PATH, \"datasets\")\n",
    "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
    "\n",
    "os.makedirs(DATASETS_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Modo:\", \"Colab\" if IN_COLAB else \"Local\")\n",
    "print(\"Ruta datasets:\", DATASETS_PATH)\n",
    "print(\"Ruta modelos:\", MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce43b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游댃 Par치metros del repo\n",
    "GIT_REPO_URL = \"https://github.com/ismaelgallolopez/ia_thermal.git\"  # 游녣 Cambia esto\n",
    "REPO_NAME = GIT_REPO_URL.split(\"/\")[-1].replace(\".git\", \"\")\n",
    "CLONE_PATH = os.path.join(BASE_PATH, REPO_NAME)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 游빏 Clonar el repositorio si no existe ya\n",
    "    if not os.path.exists(CLONE_PATH):\n",
    "        !git clone {GIT_REPO_URL} {CLONE_PATH}\n",
    "    else:\n",
    "        print(f\"Repositorio ya clonado en: {CLONE_PATH}\")\n",
    "\n",
    "    # 游닍 Instalar requirements.txt\n",
    "    req_path = os.path.join(CLONE_PATH, \"requirements.txt\")\n",
    "    if os.path.exists(req_path):\n",
    "        !pip install -r {req_path}\n",
    "    else:\n",
    "        print(\"No se encontr칩 requirements.txt en el repositorio.\")\n",
    "\n",
    "    print(\"游댃 Reinicia el entorno para aplicar los cambios...\")\n",
    "    IPython.display.display(IPython.display.Javascript('''google.colab.restartRuntime()'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7dd21",
   "metadata": {},
   "source": [
    "Importaci칩n de librer칤as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a1b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import display, Markdown\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# get the directory path of the file\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal\")\n",
    "\n",
    "from plot_functions import *\n",
    "from Physics_Loss import *\n",
    "from utils import *\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/ismaelgallo\")\n",
    "\n",
    "from architectures.mlp_pcb import *\n",
    "\n",
    "sys.path.append('../Convolutional_NN')\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/Convolutional_NN\")\n",
    "\n",
    "from Dataset_Class import *\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a84089",
   "metadata": {},
   "source": [
    "Celda para que funcione en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0236c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo: Local\n",
      "Ruta datasets: C:\\Users\\ismael.gallo/ia_thermal_colab\\datasets\n",
      "Ruta modelos: C:\\Users\\ismael.gallo/ia_thermal_colab\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "# Detectar si estamos en Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Ruta base\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = \"/content/drive/MyDrive/ia_thermal_colab\"\n",
    "else:\n",
    "    BASE_PATH = os.path.expanduser(\"~/ia_thermal_colab\")\n",
    "\n",
    "DATASETS_PATH = os.path.join(BASE_PATH, \"datasets\")\n",
    "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
    "\n",
    "os.makedirs(DATASETS_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Modo:\", \"Colab\" if IN_COLAB else \"Local\")\n",
    "print(\"Ruta datasets:\", DATASETS_PATH)\n",
    "print(\"Ruta modelos:\", MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583cda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游댃 Par치metros del repo\n",
    "GIT_REPO_URL = \"https://github.com/ismaelgallolopez/ia_thermal.git\"  # 游녣 Cambia esto\n",
    "REPO_NAME = GIT_REPO_URL.split(\"/\")[-1].replace(\".git\", \"\")\n",
    "CLONE_PATH = os.path.join(BASE_PATH, REPO_NAME)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 游빏 Clonar el repositorio si no existe ya\n",
    "    if not os.path.exists(CLONE_PATH):\n",
    "        !git clone {GIT_REPO_URL} {CLONE_PATH}\n",
    "    else:\n",
    "        print(f\"Repositorio ya clonado en: {CLONE_PATH}\")\n",
    "\n",
    "    # 游닍 Instalar requirements.txt\n",
    "    req_path = os.path.join(CLONE_PATH, \"requirements.txt\")\n",
    "    if os.path.exists(req_path):\n",
    "        !pip install -r {req_path}\n",
    "    else:\n",
    "        print(\"No se encontr칩 requirements.txt en el repositorio.\")\n",
    "\n",
    "    print(\"游댃 Reinicia el entorno para aplicar los cambios...\")\n",
    "    IPython.display.display(IPython.display.Javascript('''google.colab.restartRuntime()'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import display, Markdown\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "# import sklearn\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "# from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# get the directory path of the file\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal\")\n",
    "\n",
    "from plot_functions import *\n",
    "from Physics_Loss import *\n",
    "from utils import *\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/ismaelgallo\")\n",
    "\n",
    "from architectures.convlstm import *\n",
    "\n",
    "sys.path.append('../Convolutional_NN')\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/Convolutional_NN\")\n",
    "\n",
    "from Dataset_Class import *\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131b000",
   "metadata": {},
   "source": [
    "Configuraci칩n global de Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    # 'text.usetex': True,  # Usar LaTeX para el texto (Local)\n",
    "    'text.usetex': False,  # NO Usar LaTeX para el texto (Colab)\n",
    "    'font.family': 'serif',  # Fuente serif\n",
    "    # 'figure.figsize': (10, 6),  # Tama침o de la figura\n",
    "    'axes.labelsize': 12,  # Tama침o de las etiquetas de los ejes\n",
    "    'axes.titlesize': 14,  # Tama침o del t칤tulo\n",
    "    'legend.fontsize': 12,  # Tama침o de la leyenda\n",
    "    'xtick.labelsize': 10,  # Tama침o de las etiquetas del eje x\n",
    "    'ytick.labelsize': 10,  # Tama침o de las etiquetas del eje y\n",
    "    'axes.grid': True,  # Habilitar la cuadr칤cula\n",
    "    'grid.alpha': 0.75,  # Transparencia de la cuadr칤cula\n",
    "    'grid.linestyle': '--'  # Estilo de la l칤nea de la cuadr칤cula\n",
    "})\n",
    "\n",
    "# Configuraci칩n de Seaborn\n",
    "sns.set_context('paper')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be876f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_specs = {\n",
    "    \"os\": platform.system(),  # e.g. \"Linux\", \"Windows\", \"Darwin\"\n",
    "    \"os_version\": platform.version(),\n",
    "    \"machine\": platform.machine(),  # e.g. \"x86_64\"\n",
    "    \"processor\": platform.processor(),  # e.g. \"Intel64 Family 6 Model 158\"\n",
    "    \"python_version\": platform.python_version(),\n",
    "    \"device\": str(device)\n",
    "}\n",
    "if torch.cuda.is_available():\n",
    "    system_specs[\"gpu_name\"] = torch.cuda.get_device_name(0)\n",
    "    system_specs[\"gpu_memory_total_GB\"] = round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)\n",
    "    system_specs[\"cuda_version\"] = torch.version.cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismael_minimal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
