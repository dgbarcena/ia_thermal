{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d47121a7",
      "metadata": {
        "id": "d47121a7"
      },
      "source": [
        "# Pruebas con Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ae1c4f",
      "metadata": {
        "id": "27ae1c4f"
      },
      "source": [
        "### Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7569b95a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7569b95a",
        "outputId": "1a8b397c-3d98-456b-fa91-698833755edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modo: Local\n",
            "Ruta datasets: C:\\Users\\ismael.gallo/ia_thermal_colab\\datasets\n",
            "Ruta modelos: C:\\Users\\ismael.gallo/ia_thermal_colab\\models\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import IPython\n",
        "\n",
        "# Detectar si estamos en Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Ruta base\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = \"/content/drive/MyDrive/ia_thermal_colab\"\n",
        "else:\n",
        "    BASE_PATH = os.path.expanduser(\"~/ia_thermal_colab\")\n",
        "\n",
        "DATASETS_PATH = os.path.join(BASE_PATH, \"datasets\")\n",
        "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
        "\n",
        "os.makedirs(DATASETS_PATH, exist_ok=True)\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Modo:\", \"Colab\" if IN_COLAB else \"Local\")\n",
        "print(\"Ruta datasets:\", DATASETS_PATH)\n",
        "print(\"Ruta modelos:\", MODELS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "de808483",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "de808483",
        "outputId": "c2582acf-227b-4b27-f20a-7f0b33a22541"
      },
      "outputs": [],
      "source": [
        "# üîÑ Par√°metros del repo\n",
        "GIT_REPO_URL = \"https://github.com/ismaelgallolopez/ia_thermal.git\"  # üëà Cambia esto\n",
        "REPO_NAME = GIT_REPO_URL.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "CLONE_PATH = os.path.join(BASE_PATH, REPO_NAME)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # üß¨ Clonar el repositorio si no existe ya\n",
        "    if not os.path.exists(CLONE_PATH):\n",
        "        !git clone {GIT_REPO_URL} {CLONE_PATH}\n",
        "    else:\n",
        "        print(f\"Repositorio ya clonado en: {CLONE_PATH}\")\n",
        "\n",
        "    # üì¶ Instalar requirements.txt\n",
        "    req_path = os.path.join(CLONE_PATH, \"requirements.txt\")\n",
        "    if os.path.exists(req_path):\n",
        "        !pip install -r {req_path}\n",
        "    else:\n",
        "        print(\"No se encontr√≥ requirements.txt en el repositorio.\")\n",
        "\n",
        "    print(\"üîÑ Reinicia el entorno para aplicar los cambios...\")\n",
        "    IPython.display.display(IPython.display.Javascript('''google.colab.restartRuntime()'''))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da63d97",
      "metadata": {
        "id": "7da63d97"
      },
      "source": [
        "## Inicializar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4160dabe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4160dabe",
        "outputId": "e7d1f6f7-cabd-48b5-d020-ecc4ea38b561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ruta que usar√° SQLite: sqlite:///\\content\\drive\\MyDrive\\ia_thermal_colab\\optuna_studies\\test_study.db\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# get the directory path of the file\n",
        "dir_path = os.getcwd()\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
        "\n",
        "if IN_COLAB:\n",
        "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal\")\n",
        "\n",
        "from plot_functions import *\n",
        "from Physics_Loss import *\n",
        "from utils import *\n",
        "sys.path.append('../Convolutional_NN')\n",
        "\n",
        "if IN_COLAB:\n",
        "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/Convolutional_NN\")\n",
        "\n",
        "from Dataset_Class import *\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "  sys.path.append(\"/content/drive/MyDrive/ia_thermal_colab/ia_thermal/ismaelgallo\")\n",
        "\n",
        "from architectures.convlstm import *\n",
        "from architectures.generic_spatiotemporal_decoder import *\n",
        "from architectures.generic_spatiotemporal_regressor import *\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Ruta absoluta y segura dentro de Drive\n",
        "study_path = Path(\"/content/drive/MyDrive/ia_thermal_colab/optuna_studies\")\n",
        "study_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "db_path = study_path / \"test_study.db\"\n",
        "storage_url = f\"sqlite:///{db_path}\"\n",
        "\n",
        "print(\"Ruta que usar√° SQLite:\", storage_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "HIICVI3PGJV5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIICVI3PGJV5",
        "outputId": "ba16dc3b-88ed-4bb4-ec52-d5a65055ab7b"
      },
      "outputs": [],
      "source": [
        "# def objective(trial):\n",
        "#     x = trial.suggest_float(\"x\", -10, 10)\n",
        "#     return x ** 2\n",
        "\n",
        "# study = optuna.create_study(\n",
        "#     direction=\"minimize\",\n",
        "#     storage=storage_url,\n",
        "#     study_name=\"test_study\",\n",
        "#     load_if_exists=True\n",
        "# )\n",
        "\n",
        "# study.optimize(objective, n_trials=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "kbsWVc7LGMU5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbsWVc7LGMU5",
        "outputId": "bb560d97-f48a-4fc6-bee8-fdd3f766e4f3"
      },
      "outputs": [],
      "source": [
        "# print(\"‚úÖ ¬øExiste el archivo?\", db_path.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a69e9ffd",
      "metadata": {
        "id": "a69e9ffd"
      },
      "outputs": [],
      "source": [
        "epochs = 500\n",
        "n_train = 1000\n",
        "n_val = 200\n",
        "\n",
        "sequence_length = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e679264a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e679264a",
        "outputId": "5a2dc75f-7978-44fa-a803-caf3e63500b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cargando dataset train desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_transient_dataset_train.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\../Convolutional_NN\\Dataset_Class.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  base_dataset = torch.load(full_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cargando dataset val desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_transient_dataset_val.pth\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  dir_path = BASE_PATH\n",
        "\n",
        "# ‚¨ÖÔ∏è Esto se ejecuta una vez\n",
        "dataset_train = load_trimmed_dataset(\n",
        "    base_path=dir_path,\n",
        "    dataset_type='train',\n",
        "    max_samples=n_train,\n",
        "    time_steps_output=sequence_length\n",
        ")\n",
        "dataset_val = load_trimmed_dataset(\n",
        "    base_path=dir_path,\n",
        "    dataset_type='val',\n",
        "    max_samples=n_val,\n",
        "    time_steps_output=sequence_length\n",
        ")\n",
        "\n",
        "# Convertir a tensores de entrada y salida\n",
        "input_train, output_train = prepare_data_for_convlstm(dataset_train, device='cpu')\n",
        "input_val, output_val = prepare_data_for_convlstm(dataset_val, device='cpu')\n",
        "\n",
        "\n",
        "def get_data_loaders_from_tensors(batch_size):\n",
        "    train_loader = DataLoader(TensorDataset(input_train, output_train), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(input_val, output_val), batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d2d27df9",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_SAVE_DIR = \"saved_models\"\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd475b6c",
      "metadata": {
        "id": "dd475b6c"
      },
      "source": [
        "## ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cE5MyqliEwkB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE5MyqliEwkB",
        "outputId": "a94b76c7-eee9-459a-cc2a-d33b1f2de039"
      },
      "outputs": [],
      "source": [
        "# print(\"Ruta completa del .db:\", storage_url.replace(\"sqlite:///\", \"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bz1cW5TfE1p5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz1cW5TfE1p5",
        "outputId": "dddfb21b-e2b3-4c33-8da1-bab43ffa8529"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# print(\"¬øExiste el archivo?:\", os.path.exists(storage_url.replace(\"sqlite:///\", \"\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSn6cDyuFJ75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSn6cDyuFJ75",
        "outputId": "3dec4060-472a-4fd0-f8ae-7e6a34ef31d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-06 12:36:43,959] A new study created in RDB with name: test_study\n",
            "[I 2025-05-06 12:36:45,626] Trial 0 finished with value: 4.7560397290313965 and parameters: {'x': -2.180834640460252}. Best is trial 0 with value: 4.7560397290313965.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "    return trial.suggest_float(\"x\", -10, 10) ** 2\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    storage=\"sqlite:////content/drive/MyDrive/ia_thermal_colab/optuna_studies/test_dummy.db\",\n",
        "    study_name=\"test_study\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "study.optimize(objective, n_trials=1)\n",
        "\n",
        "import os\n",
        "print(os.path.exists(\"/content/drive/MyDrive/ia_thermal_colab/optuna_studies/test_dummy.db\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c25d49d",
      "metadata": {
        "id": "5c25d49d"
      },
      "outputs": [],
      "source": [
        "class ConvLSTMWrapper(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convlstm = ConvLSTM(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            kernel_size=kernel_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bias=True,\n",
        "            return_all_layers=False\n",
        "        )\n",
        "\n",
        "        self.output_conv = nn.Conv2d(\n",
        "            in_channels=hidden_dim[-1],\n",
        "            out_channels=1,\n",
        "            kernel_size=1  # ‚úÖ produce salida de 1 canal sin cambiar tama√±o\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: [B, T, C, 13, 13]\n",
        "        lstm_output, _ = self.convlstm(x)  # lstm_output is [layer_output_list]\n",
        "        y = lstm_output[0]  # [B, T, hidden_dim[-1], 13, 13]\n",
        "\n",
        "        # Aplicar conv final a cada paso temporal\n",
        "        B, T, C, H, W = y.shape\n",
        "        y = y.view(B * T, C, H, W)\n",
        "        y = self.output_conv(y)             # [B*T, 1, 13, 13]\n",
        "        y = y.view(B, T, 1, H, W)           # [B, T, 1, 13, 13]\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6764169f",
      "metadata": {
        "id": "6764169f"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Hiperpar√°metros\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
        "    hidden_dim_val = trial.suggest_categorical(\"hidden_dim\", [16, 32, 64, 128])\n",
        "    kernel_size_val = trial.suggest_categorical(\"kernel_size\", [1, 3, 5, 7])\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
        "\n",
        "    # Adaptar al formato requerido por ConvLSTM\n",
        "    hidden_dim = [hidden_dim_val] * num_layers\n",
        "    kernel_size = [(kernel_size_val, kernel_size_val)] * num_layers\n",
        "\n",
        "    # Crear modelo ConvLSTM\n",
        "    model = ConvLSTMWrapper(\n",
        "        input_dim=3,  # o el n√∫mero de canales reales\n",
        "        hidden_dim=hidden_dim,\n",
        "        kernel_size=kernel_size,\n",
        "        num_layers=num_layers\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Obtener dataloaders (train y val)\n",
        "    train_loader, val_loader = get_data_loaders_from_tensors(batch_size=batch_size)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, epochs)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "        \n",
        "    # Guardar el mejor modelo del trial\n",
        "    model_path = os.path.join(MODEL_SAVE_DIR, f\"model_trial_{trial.number}.pth\")\n",
        "    torch.save(best_model_state, model_path)    \n",
        "    \n",
        "    # Guardar los hiperpar√°metros del trial en formato JSON\n",
        "    param_path = os.path.join(MODEL_SAVE_DIR, f\"params_trial_{trial.number}.json\")\n",
        "    with open(param_path, \"w\") as f:\n",
        "        json.dump(trial.params, f, indent=2)\n",
        "    \n",
        "    return best_val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a861fab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a861fab",
        "outputId": "582ac60b-47e1-4eba-abd3-63b1749faa14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-06 12:37:07,487] Using an existing study with name 'convlstm' instead of creating a new one.\n",
            "Epoch 1/10 - Training:   0%|          | 0/16 [00:00<?, ?it/s]c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([64, 1, 20, 13, 13])) that is different to the input size (torch.Size([64, 1, 1, 13, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n",
            "Epoch 1/10 - Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:00<00:00, 44.95it/s, loss=0.395]c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([40, 1, 20, 13, 13])) that is different to the input size (torch.Size([40, 1, 1, 13, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n",
            "[I 2025-05-06 12:37:10,710] Trial 1 finished with value: 0.05529384035617113 and parameters: {'lr': 0.00837817254286601, 'batch_size': 64, 'hidden_dim': 64, 'kernel_size': 1, 'num_layers': 3}. Best is trial 1 with value: 0.05529384035617113.\n",
            "[I 2025-05-06 12:37:12,042] Trial 2 finished with value: 0.8635045737028122 and parameters: {'lr': 0.00029795286210330935, 'batch_size': 64, 'hidden_dim': 32, 'kernel_size': 3, 'num_layers': 1}. Best is trial 1 with value: 0.05529384035617113.\n",
            "[I 2025-05-06 12:37:14,761] Trial 3 finished with value: 0.05267427010195596 and parameters: {'lr': 0.0011975923071317865, 'batch_size': 32, 'hidden_dim': 16, 'kernel_size': 3, 'num_layers': 3}. Best is trial 3 with value: 0.05267427010195596.\n",
            "[I 2025-05-06 12:37:25,204] Trial 4 finished with value: 0.0755251981317997 and parameters: {'lr': 0.0011814609936976368, 'batch_size': 64, 'hidden_dim': 64, 'kernel_size': 5, 'num_layers': 4}. Best is trial 3 with value: 0.05267427010195596.\n",
            "[W 2025-05-06 12:38:44,977] Trial 5 failed with parameters: {'lr': 0.0022345783894975506, 'batch_size': 64, 'hidden_dim': 128, 'kernel_size': 7, 'num_layers': 3} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\ismael.gallo\\AppData\\Local\\Temp\\ipykernel_2892\\503547808.py\", line 30, in objective\n",
            "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, epochs)\n",
            "  File \"c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\utils.py\", line 153, in train_one_epoch\n",
            "    total_loss += loss.item()\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-06 12:38:44,977] Trial 5 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[68], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m      2\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvlstm\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# puedes elegirlo t√∫\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(study_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudy_convlstm.db\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# üß† crea archivo .db en tu carpeta\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# por si ya existe, contin√∫a donde lo dejaste\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[67], line 30\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     28\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 30\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[0;32m     32\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
            "File \u001b[1;32mc:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\utils.py:153\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device, epoch, total_epochs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    151\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 153\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"convlstm\",  # puedes elegirlo t√∫\n",
        "    storage=\"sqlite:///\" + os.path.join(study_path, \"study_convlstm.db\"),  # üß† crea archivo .db en tu carpeta\n",
        "    load_if_exists=True  # por si ya existe, contin√∫a donde lo dejaste\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "358d1f45",
      "metadata": {
        "id": "358d1f45"
      },
      "outputs": [],
      "source": [
        "print(\"Best trial:\")\n",
        "for key, val in study.best_trial.params.items():\n",
        "    print(f\"{key}: {val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e950fcd",
      "metadata": {
        "id": "0e950fcd"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study).show()\n",
        "optuna.visualization.plot_param_importances(study).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79656890",
      "metadata": {
        "id": "79656890"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac41ec4",
      "metadata": {
        "id": "5ac41ec4"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Hiperpar√°metros estructurales\n",
        "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 256, 512])\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 6)\n",
        "    nhead = trial.suggest_categorical(\"nhead\", [1, 2, 4, 8])\n",
        "    dim_feedforward_factor = trial.suggest_int(\"dim_ff_factor\", 2, 6)\n",
        "    use_temporal_channel = trial.suggest_categorical(\"use_temporal_channel\", [False, True])\n",
        "\n",
        "    # Hiperpar√°metros de entrenamiento\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
        "\n",
        "    # Elegir optimizador\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"RMSprop\"])\n",
        "\n",
        "    # Dataloaders\n",
        "    train_loader, val_loader = get_data_loaders_from_tensors(batch_size)\n",
        "\n",
        "    # Modelo\n",
        "    class CustomTransformerDecoder(TransformerDecoder):\n",
        "        def __init__(self, embedding_dim, num_layers, nhead, dim_ff, dropout):\n",
        "            super().__init__(embedding_dim, num_layers, nhead)\n",
        "            encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_dim,\n",
        "                nhead=nhead,\n",
        "                dim_feedforward=dim_ff,\n",
        "                dropout=dropout,\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    model = GenericSpatioTemporalDecoder(\n",
        "        embedding_dim=embedding_dim,\n",
        "        num_layers=num_layers,\n",
        "        nhead=nhead,\n",
        "        in_channels=3,\n",
        "        use_temporal_channel=use_temporal_channel\n",
        "    ).to(device)\n",
        "\n",
        "    # Reemplazar el decoder por uno con dropout y tama√±o FF ajustado\n",
        "    model.temporal_decoder = CustomTransformerDecoder(\n",
        "        embedding_dim=embedding_dim,\n",
        "        num_layers=num_layers,\n",
        "        nhead=nhead,\n",
        "        dim_ff=embedding_dim * dim_feedforward_factor,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    # Optimizador\n",
        "    optimizer_cls = {\"Adam\": torch.optim.Adam, \"AdamW\": torch.optim.AdamW, \"RMSprop\": torch.optim.RMSprop}[optimizer_name]\n",
        "    optimizer = optimizer_cls(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Entrenamiento\n",
        "    best_val_loss = float(\"inf\")\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, epochs)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93468e3",
      "metadata": {
        "id": "e93468e3"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"decoder1\",  # puedes elegirlo t√∫\n",
        "    storage=storage_url,  # üß† crea archivo .db en tu carpeta\n",
        "    load_if_exists=True  # por si ya existe, contin√∫a donde lo dejaste\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Mostrar el mejor resultado\n",
        "print(\"üîç Best trial:\")\n",
        "for k, v in study.best_trial.params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d69a161",
      "metadata": {
        "id": "3d69a161"
      },
      "outputs": [],
      "source": [
        "import optuna.visualization as vis\n",
        "\n",
        "vis.plot_optimization_history(study).show()\n",
        "vis.plot_param_importances(study).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71120961",
      "metadata": {
        "id": "71120961"
      },
      "source": [
        "## Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc94e0b3",
      "metadata": {
        "id": "fc94e0b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cargando dataset train desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_transient_dataset_train.pth\n",
            "‚úÖ Cargando dataset val desde: c:\\Users\\ismael.gallo\\Desktop\\ia_thermal\\ismaelgallo\\datasets\\PCB_transient_dataset_val.pth\n"
          ]
        }
      ],
      "source": [
        "# Cargar los datos una sola vez\n",
        "dataset_train = load_trimmed_dataset(\n",
        "    base_path=dir_path, dataset_type='train',\n",
        "    max_samples=n_train, time_steps_output=sequence_length\n",
        ")\n",
        "dataset_val = load_trimmed_dataset(\n",
        "    base_path=dir_path, dataset_type='val',\n",
        "    max_samples=n_val, time_steps_output=sequence_length\n",
        ")\n",
        "\n",
        "x_train, y_train = prepare_data_for_convlstm(dataset_train, device='cpu')\n",
        "x_val, y_val = prepare_data_for_convlstm(dataset_val, device='cpu')\n",
        "\n",
        "train_dataset = TemporalRegressionDataset(x_train, y_train)\n",
        "val_dataset = TemporalRegressionDataset(x_val, y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6da1a429",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_SAVE_DIR_1 = os.path.join(MODEL_SAVE_DIR, \"regressor\")\n",
        "os.makedirs(MODEL_SAVE_DIR_1, exist_ok=True)  # Ensure the directory exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "66140c54",
      "metadata": {
        "id": "66140c54"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Hiperpar√°metros\n",
        "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 256])\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
        "    nhead = trial.suggest_categorical(\"nhead\", [1, 2, 4])\n",
        "    dim_ff_factor = trial.suggest_int(\"dim_ff_factor\", 2, 6)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"RMSprop\"])\n",
        "\n",
        "    # DataLoaders desde datasets pre-cargados\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Modelo\n",
        "    model = GenericSpatioTemporalRegressor(\n",
        "        embedding_dim=embedding_dim,\n",
        "        num_layers=num_layers,\n",
        "        nhead=nhead,\n",
        "        in_channels=3\n",
        "    ).to(device)\n",
        "\n",
        "    # Reemplazo del decoder por uno personalizado con dropout y FF dimensionado\n",
        "    class CustomTransformerDecoder(TransformerDecoder):\n",
        "        def __init__(self, embedding_dim, num_layers, nhead, dim_ff, dropout):\n",
        "            super().__init__(embedding_dim, num_layers, nhead)\n",
        "            encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_dim,\n",
        "                nhead=nhead,\n",
        "                dim_feedforward=dim_ff,\n",
        "                dropout=dropout,\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    model.temporal_decoder = CustomTransformerDecoder(\n",
        "        embedding_dim=embedding_dim * 2,\n",
        "        num_layers=num_layers,\n",
        "        nhead=nhead,\n",
        "        dim_ff=embedding_dim * dim_ff_factor,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    # Optimizador\n",
        "    optimizer_cls = {\"Adam\": torch.optim.Adam, \"AdamW\": torch.optim.AdamW, \"RMSprop\": torch.optim.RMSprop}[optimizer_name]\n",
        "    optimizer = optimizer_cls(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, epochs)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "        \n",
        "    # Guardar el mejor modelo del trial\n",
        "    model_path = os.path.join(MODEL_SAVE_DIR_1, f\"model_trial_{trial.number}.pth\")\n",
        "    torch.save(best_model_state, model_path)    \n",
        "    \n",
        "    # Guardar los hiperpar√°metros del trial en formato JSON\n",
        "    param_path = os.path.join(MODEL_SAVE_DIR_1, f\"params_trial_{trial.number}.json\")\n",
        "    with open(param_path, \"w\") as f:\n",
        "        json.dump(trial.params, f, indent=2)\n",
        "    \n",
        "    return best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "caabb460",
      "metadata": {
        "id": "caabb460"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-07 12:08:35,339] Using an existing study with name 'regressor' instead of creating a new one.\n",
            "Epoch 1/500 - Training:   0%|          | 0/63 [00:00<?, ?it/s]c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 1, 20, 13, 13])) that is different to the input size (torch.Size([16, 1, 1, 13, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Epoch 1/500 - Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:01<00:00, 62.27it/s, loss=0.05]  c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([8, 1, 20, 13, 13])) that is different to the input size (torch.Size([8, 1, 1, 13, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "[I 2025-05-07 12:15:38,379] Trial 29 finished with value: 0.014833396014112692 and parameters: {'embedding_dim': 128, 'num_layers': 1, 'nhead': 2, 'dim_ff_factor': 6, 'dropout': 0.03459477259865107, 'batch_size': 16, 'lr': 0.00032003978241631277, 'weight_decay': 0.007675174931438396, 'optimizer': 'AdamW'}. Best is trial 27 with value: 0.014823907054960728.\n",
            "[I 2025-05-07 12:15:39,743] Trial 30 pruned.                                        \n",
            "[I 2025-05-07 12:22:42,805] Trial 31 finished with value: 0.01482851065408725 and parameters: {'embedding_dim': 128, 'num_layers': 1, 'nhead': 2, 'dim_ff_factor': 5, 'dropout': 0.0790775386191188, 'batch_size': 16, 'lr': 0.0006040979642070929, 'weight_decay': 0.0014208561889369697, 'optimizer': 'AdamW'}. Best is trial 27 with value: 0.014823907054960728.\n",
            "[I 2025-05-07 12:22:53,474] Trial 32 pruned.                                        \n",
            "Epoch 1/500 - Training:   0%|          | 0/32 [00:00<?, ?it/s]c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 1, 20, 13, 13])) that is different to the input size (torch.Size([32, 1, 1, 13, 13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "[I 2025-05-07 12:22:54,466] Trial 33 pruned.                                        \n",
            "[I 2025-05-07 12:30:57,845] Trial 34 finished with value: 0.01483762701256917 and parameters: {'embedding_dim': 128, 'num_layers': 1, 'nhead': 2, 'dim_ff_factor': 5, 'dropout': 0.030179202920710725, 'batch_size': 16, 'lr': 0.0003522786132740377, 'weight_decay': 0.0006231787324799432, 'optimizer': 'AdamW'}. Best is trial 27 with value: 0.014823907054960728.\n",
            "[I 2025-05-07 12:31:00,096] Trial 35 pruned.                                        \n",
            "[I 2025-05-07 12:31:01,490] Trial 36 pruned.                                        \n",
            "[I 2025-05-07 12:31:03,014] Trial 37 pruned.                                        \n",
            "[I 2025-05-07 12:31:04,107] Trial 38 pruned.                                        \n",
            "[I 2025-05-07 12:31:05,189] Trial 39 pruned.                                        \n",
            "[I 2025-05-07 12:31:06,270] Trial 40 pruned.                                        \n",
            "c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(\n",
            "[I 2025-05-07 12:31:07,407] Trial 41 pruned.                                        \n",
            "[I 2025-05-07 12:31:09,162] Trial 42 pruned.                                        \n",
            "[I 2025-05-07 12:31:20,115] Trial 43 pruned.                                         \n",
            "[I 2025-05-07 12:31:20,915] Trial 44 pruned.                                        \n",
            "[I 2025-05-07 12:31:22,546] Trial 45 pruned.                                          \n",
            "[I 2025-05-07 12:31:26,279] Trial 46 pruned.                                          \n",
            "[I 2025-05-07 12:31:27,646] Trial 47 pruned.                                        \n",
            "[I 2025-05-07 12:31:38,055] Trial 48 pruned.                                         \n",
            "[I 2025-05-07 12:31:46,683] Trial 49 pruned.                                        \n",
            "[I 2025-05-07 12:31:49,720] Trial 50 pruned.                                        \n",
            "[I 2025-05-07 12:31:50,820] Trial 51 pruned.                                        \n",
            "[I 2025-05-07 12:31:51,591] Trial 52 pruned.                                        \n",
            "[I 2025-05-07 12:31:53,301] Trial 53 pruned.                                          \n",
            "[I 2025-05-07 12:31:54,382] Trial 54 pruned.                                        \n",
            "[I 2025-05-07 12:31:55,584] Trial 55 pruned.                                        \n",
            "[I 2025-05-07 12:31:57,559] Trial 56 pruned.                                        \n",
            "[I 2025-05-07 12:31:58,649] Trial 57 pruned.                                        \n",
            "[I 2025-05-07 12:32:07,068] Trial 58 pruned.                                        \n",
            "[I 2025-05-07 12:32:17,519] Trial 59 pruned.                                         \n",
            "[I 2025-05-07 12:32:19,533] Trial 60 pruned.                                        \n",
            "[I 2025-05-07 12:32:31,123] Trial 61 pruned.                                        \n",
            "[I 2025-05-07 12:32:32,736] Trial 62 pruned.                                        \n",
            "[I 2025-05-07 12:32:36,084] Trial 63 pruned.                                          \n",
            "[I 2025-05-07 12:32:47,636] Trial 64 pruned.                                         \n",
            "[I 2025-05-07 12:32:48,405] Trial 65 pruned.                                        \n",
            "[I 2025-05-07 12:32:49,754] Trial 66 pruned.                                        \n",
            "[I 2025-05-07 12:32:51,963] Trial 67 pruned.                                        \n",
            "[I 2025-05-07 12:33:02,923] Trial 68 pruned.                                         \n",
            "[I 2025-05-07 12:33:13,744] Trial 69 pruned.                                         \n",
            "[I 2025-05-07 12:33:15,843] Trial 70 pruned.                                        \n",
            "[I 2025-05-07 12:33:28,374] Trial 71 pruned.                                         \n",
            "[I 2025-05-07 12:33:30,448] Trial 72 pruned.                                        \n",
            "[I 2025-05-07 12:33:46,034] Trial 73 pruned.                                         \n",
            "[I 2025-05-07 12:33:46,899] Trial 74 pruned.                                         \n",
            "[I 2025-05-07 12:33:47,757] Trial 75 pruned.                                        \n",
            "[I 2025-05-07 12:33:48,454] Trial 76 pruned.                                        \n",
            "c:\\Users\\ismael.gallo\\anaconda3\\envs\\ismael_minimal\\lib\\site-packages\\optuna\\pruners\\_percentile.py:21: RuntimeWarning: All-NaN slice encountered\n",
            "  return np.nanmin(values)\n",
            "[I 2025-05-07 12:33:50,417] Trial 77 pruned. \n",
            "[I 2025-05-07 12:34:54,562] Trial 78 pruned.                                            \n",
            "[I 2025-05-07 12:36:07,324] Trial 79 pruned.                                            \n",
            "[I 2025-05-07 12:36:26,894] Trial 80 pruned.                                           \n",
            "[I 2025-05-07 12:37:53,640] Trial 81 pruned.                                           \n",
            "[I 2025-05-07 12:37:54,948] Trial 82 pruned.                                        \n",
            "[I 2025-05-07 12:37:56,238] Trial 83 pruned.                                        \n",
            "[I 2025-05-07 12:38:07,029] Trial 84 pruned.                                        \n",
            "[I 2025-05-07 12:38:08,525] Trial 85 pruned.                                        \n",
            "[I 2025-05-07 12:39:25,650] Trial 86 pruned.                                            \n",
            "[I 2025-05-07 12:39:26,964] Trial 87 pruned.                                        \n",
            "[I 2025-05-07 12:39:28,596] Trial 88 pruned.                                          \n",
            "[I 2025-05-07 12:39:29,913] Trial 89 pruned.                                           \n",
            "[I 2025-05-07 12:39:33,009] Trial 90 pruned.                                          \n",
            "[I 2025-05-07 12:39:35,529] Trial 91 pruned.                                           \n",
            "[I 2025-05-07 12:39:36,310] Trial 92 pruned.                                        \n",
            "[I 2025-05-07 12:39:37,173] Trial 93 pruned.                                         \n",
            "[I 2025-05-07 12:39:38,024] Trial 94 pruned.                                        \n",
            "[I 2025-05-07 12:39:38,930] Trial 95 pruned.                                        \n",
            "[I 2025-05-07 12:39:41,525] Trial 96 pruned.                                           \n",
            "[I 2025-05-07 12:39:42,599] Trial 97 pruned.                                        \n",
            "[I 2025-05-07 12:39:44,524] Trial 98 pruned.                                          \n",
            "[I 2025-05-07 12:39:59,714] Trial 99 pruned.                                          \n",
            "[I 2025-05-07 12:40:01,648] Trial 100 pruned.                                         \n",
            "[I 2025-05-07 12:40:03,828] Trial 101 pruned.                                         \n",
            "[I 2025-05-07 12:40:04,884] Trial 102 pruned.                                       \n",
            "[I 2025-05-07 12:40:06,741] Trial 103 pruned.                                       \n",
            "[I 2025-05-07 12:40:07,879] Trial 104 pruned.                                       \n",
            "[I 2025-05-07 12:40:10,151] Trial 105 pruned.                                       \n",
            "[I 2025-05-07 12:40:14,084] Trial 106 pruned.                                         \n",
            "[I 2025-05-07 12:40:27,234] Trial 107 pruned.                                        \n",
            "[I 2025-05-07 12:40:30,384] Trial 108 pruned.                                         \n",
            "[I 2025-05-07 12:40:35,131] Trial 109 pruned.                                         \n",
            "[I 2025-05-07 12:40:37,428] Trial 110 pruned.                                         \n",
            "[I 2025-05-07 12:40:39,984] Trial 111 pruned.                                          \n",
            "[I 2025-05-07 12:40:41,563] Trial 112 pruned.                                       \n",
            "[I 2025-05-07 12:40:42,605] Trial 113 pruned.                                       \n",
            "[I 2025-05-07 12:40:44,571] Trial 114 pruned.                                         \n",
            "[I 2025-05-07 12:40:46,543] Trial 115 pruned.                                       \n",
            "[I 2025-05-07 12:40:47,414] Trial 116 pruned.                                        \n",
            "[I 2025-05-07 12:40:50,054] Trial 117 pruned.                                          \n",
            "[I 2025-05-07 12:40:51,140] Trial 118 pruned.                                       \n",
            "[I 2025-05-07 12:40:52,194] Trial 119 pruned.                                       \n",
            "[I 2025-05-07 12:40:53,237] Trial 120 pruned.                                       \n",
            "[I 2025-05-07 12:40:54,029] Trial 121 pruned.                                       \n",
            "[I 2025-05-07 12:40:54,612] Trial 122 pruned.                                       \n",
            "[I 2025-05-07 12:41:03,574] Trial 123 pruned.                                        \n",
            "[I 2025-05-07 12:41:04,944] Trial 124 pruned.                                       \n",
            "[I 2025-05-07 12:41:05,828] Trial 125 pruned.                                        \n",
            "[I 2025-05-07 12:41:08,865] Trial 126 pruned.                                         \n",
            "[I 2025-05-07 12:41:10,079] Trial 127 pruned.                                       \n",
            "[I 2025-05-07 12:41:12,698] Trial 128 pruned.                                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Best trial:\n",
            "embedding_dim: 128\n",
            "num_layers: 1\n",
            "nhead: 2\n",
            "dim_ff_factor: 6\n",
            "dropout: 0.013352905265470116\n",
            "batch_size: 16\n",
            "lr: 0.0007568347034202091\n",
            "weight_decay: 0.009836898376414951\n",
            "optimizer: AdamW\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"regressor\",  # puedes elegirlo t√∫\n",
        "    storage=\"sqlite:///optuna_studies/study_regressor.db\",  # üß† crea archivo .db en tu carpeta\n",
        "    load_if_exists=True  # por si ya existe, contin√∫a donde lo dejaste\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Mostrar el mejor resultado\n",
        "print(\"üîç Best trial:\")\n",
        "for k, v in study.best_trial.params.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cce7c184",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CustomTransformerDecoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m GenericSpatioTemporalRegressor(\n\u001b[0;32m     15\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     17\u001b[0m     nhead\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhead\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Si usas un decoder personalizado:\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mtemporal_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTransformerDecoder\u001b[49m(\n\u001b[0;32m     22\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     23\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     24\u001b[0m     nhead\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhead\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     25\u001b[0m     dim_ff\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_ff_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     26\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     27\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Cargar pesos entrenados\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(best_model_path))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'CustomTransformerDecoder' is not defined"
          ]
        }
      ],
      "source": [
        "# Recuperar el mejor trial\n",
        "best_trial = study.best_trial\n",
        "best_trial_number = best_trial.number\n",
        "\n",
        "# Rutas del mejor modelo y sus par√°metros\n",
        "best_model_path = f\"saved_models/regressor/model_trial_{best_trial_number}.pth\"\n",
        "best_param_path = f\"saved_models/regressor/params_trial_{best_trial_number}.json\"\n",
        "\n",
        "# Cargar los par√°metros del mejor modelo\n",
        "with open(best_param_path, \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Crear el modelo con los mejores hiperpar√°metros\n",
        "model = GenericSpatioTemporalRegressor(\n",
        "    embedding_dim=best_params[\"embedding_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    nhead=best_params[\"nhead\"]\n",
        ").to(device)\n",
        "\n",
        "# Si usas un decoder personalizado:\n",
        "model.temporal_decoder = CustomTransformerDecoder(\n",
        "    embedding_dim=best_params[\"embedding_dim\"] * 2,\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    nhead=best_params[\"nhead\"],\n",
        "    dim_ff=best_params[\"embedding_dim\"] * best_params[\"dim_ff_factor\"] * 2,\n",
        "    dropout=best_params[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "# Cargar pesos entrenados\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "# Guardar como el mejor modelo global\n",
        "torch.save(model.state_dict(), \"best_model_of_study.pth\")\n",
        "with open(\"best_model_of_study_params.json\", \"w\") as f:\n",
        "    json.dump(best_params, f, indent=2)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ismael_minimal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
